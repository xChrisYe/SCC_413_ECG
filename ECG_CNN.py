
# -*- coding: utf-8 -*-
"""ECG_Experiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LeOZXR-ubev-IXBGUkuyFpSwPotqvNk6

"""

"""### Download and import the dataset """

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
np.random.seed(0)
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from tensorflow.keras import layers, losses
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Model
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix



dataframe = pd.read_csv('http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv', header=None)
raw_data = dataframe.values
dataframe.head()

# The last element contains the labels
labels = raw_data[:, -1]

# The other data points are the electrocadriogram data
data = raw_data[:, 0:-1]

train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=21)

# Normalize to [0, 1]
min_val = tf.reduce_min(train_data)
max_val = tf.reduce_max(train_data)

train_data = (train_data - min_val) / (max_val - min_val)
test_data = (test_data - min_val) / (max_val - min_val)

train_data = tf.cast(train_data, tf.float32)
test_data = tf.cast(test_data, tf.float32)


from keras.callbacks import EarlyStopping

# Plot function to show the loss_curve of history 
def plot_loss(loss,val_loss):
  plt.figure()
  plt.plot(loss)
  plt.plot(val_loss)
  plt.title('Model loss')
  plt.ylabel('Loss')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Test'], loc='upper right')
  plt.show()

# EarlyStopping function to end epcho
monitor_val_loss = EarlyStopping(monitor='val_loss', patience=3)


"""### Use CNN method to model"""
# Transfor tensfor to numpy
train_data=np.array(train_data)
test_data=np.array(test_data)
# Conv1D

from keras.layers import Conv1D,MaxPooling1D,Dropout,Flatten

# transform into suitable shape
train_data_cnn = train_data.reshape(-1, 140, 1)
test_data_cnn = test_data.reshape(-1, 140, 1)


# Build CNN architecture
CNN = Sequential()
CNN.add(Conv1D(16, 10, activation='tanh', input_shape=(140,1)))
CNN.add(Conv1D(16, 10, activation='tanh'))
CNN.add(MaxPooling1D(3))
CNN.add(Dropout(0.25))
CNN.add(Conv1D(32, 10, activation='tanh'))
CNN.add(Conv1D(32, 10, activation='tanh'))
CNN.add(MaxPooling1D(3))
CNN.add(Dropout(0.25))

CNN.add(Flatten())

CNN.add(Dense(128,activation='tanh'))
CNN.add(Dropout(0.25))
CNN.add(Dense(1, activation='sigmoid'))
CNN.summary()

CNN.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=opt)

hist_cnn = CNN.fit(train_data_cnn,train_labels, batch_size=14, epochs=50, validation_split=0.2, callbacks=[monitor_val_loss])

# Plot loss curve
plot_loss(hist_cnn.history['loss'], hist_cnn.history['val_loss'])

# Show test result
CNN.evaluate(test_data_cnn,test_labels)

# Predict and mark
y_pred_cnn=CNN.predict_classes(test_data_cnn)
print(f1_score(test_labels, y_pred_cnn, average="macro"))
print(precision_score(test_labels, y_pred_cnn, average="macro"))
print(recall_score(test_labels, y_pred_cnn, average="macro"))




"""### Plot val_loss and vol_acc plot"""

plt.figure()

plt.plot(hist_cnn.history['val_loss'])

plt.title('Model val_loss')
plt.ylabel('val_loss')
plt.xlabel('Epoch')
plt.legend([ 'cnn'], loc='upper right')
plt.show()

plt.figure()

plt.plot(hist_cnn.history['val_accuracy'])

plt.title('Model val_accuracy')
plt.ylabel('val_accuracy')
plt.xlabel('Epoch')
plt.legend([ 'cnn'], loc='upper right')
plt.show()
